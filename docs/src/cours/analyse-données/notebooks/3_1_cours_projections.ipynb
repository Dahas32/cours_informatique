{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projections\n",
    "\n",
    "Alternative aux régressions linéaires pour l'analyse des données.\n",
    "\n",
    "## Données\n",
    "\n",
    "On considère qu'une donnée $x_i$ est décrite par $m$ caractéristiques réelles. La donnée $x_i$ est ainsi assimilable à un vecteur $x_i = (x_i^1, \\dots, x_i^m)$ de $\\mathbb{R}^m$. Si l'on possède $n$ données, on peut les représenter sous la forme d'une matrice :\n",
    "\n",
    "$$X = \\left(\n",
    "\\begin{array}{cccccc}\n",
    "x^1_1&\\dots &x^j_1 &\\dots &x_1^m\\\\\n",
    "     &      &\\vdots&      &  \\\\\n",
    "x_i^1&\\dots &x^j_i &\\dots &x_i^m\\\\\n",
    "     &      &\\vdots&      & \\\\\n",
    "x_n^1&\\dots &x^j_n&\\dots &x_n^m\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "Où :\n",
    "\n",
    "* une donnée est un vecteur ligne $x_i$ à $m$ coordonnées\n",
    "* un caractère est un vecteur colonne  $x^j$  à $n$ coordonnées\n",
    "\n",
    "L'analyse de données numériques utilisant souvent des méthode géométriques (les données $x_i$ sont des points d'un espace vectoriel $\\mathbb{R}^m$), on aura tendance à :\n",
    "\n",
    "* assimiler les données à des points\n",
    "* nommer  ***nuage de points** l'ensemble de nos données\n",
    "* le point $g = (\\overline{x^1}, \\dots, \\overline{x^j}, \\dots, overline{x^m})$ constitué des moyennes de chaque caractère est nommé ***centre de gravité du nuage***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du problème\n",
    "\n",
    "On cherche à trouver la droite $\\mathcal{D}$ qui minimise globalement la projection de nos données sur celle-ci. De façon analogue à la régression on cherche à minimiser globalement l'erreur entre le point réel $x$ et sont approximé $p_\\mathcal{D}(x)$ (la projection de de $x$ sur $\\mathcal{D}$) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "display.display(display.Image(\"projection-opti.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La droite minimisant les projection de nos $n$ données $x_i$ est alors la droite $\\mathcal{D}$ qui minimise la fonction :\n",
    "\n",
    "$$\n",
    "f(\\mathcal{D}) = \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||x_i - p_\\mathcal{D}(x_i)||_2^2 = \\sum_{1 \\leq i \\leq n} \\frac{1}{n} (x_i - p_\\mathcal{D}(x_i))^2\n",
    "$$\n",
    "\n",
    "On peut manipuler cette équation pour en savoir plus sur cette droite optimale, que l'on appellera $\\mathcal{D}^\\star$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse du problème\n",
    "\n",
    "On remarque que $x_i$, $p_\\mathcal{D}(x_i)$ et $p_\\mathcal{D}(g)$ ($g$ est le centre de gravité du nuage) forment un triangle rectangle en $p_\\mathcal{D}(x_i)$ puisque $p_\\mathcal{D}(g)$ est sur la droite. Pythagore nous indique alors que :\n",
    "\n",
    "$$\n",
    "||p_\\mathcal{D}(g) - x_i||_2^2 = ||p_\\mathcal{D}(x_i) - x_i||_2^2 + ||p_\\mathcal{D}(x_i) - p_\\mathcal{D}(g)||_2^2\n",
    "$$\n",
    "\n",
    "De là : \n",
    "\n",
    "$$\n",
    "f(\\mathcal{D}) = \\sum_{1 \\leq i \\leq n}  \\frac{1}{n}||x_i - p_\\mathcal{D}(x_i)||_2^2 = \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||p_\\mathcal{D}(g) - x_i||_2^2 - \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||p_\\mathcal{D}(x_i) - p_\\mathcal{D}(g)||_2^2\n",
    "$$\n",
    "\n",
    "En appliquant le [théorème de König-Huygens](https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_de_K%C3%B6nig-Huygens#%C3%89nonc%C3%A9_en_statistiques) qui stipule que :\n",
    "\n",
    "$$\n",
    "\\sum_{1 \\leq i \\leq n} \\frac{1}{n}||x_i - p_\\mathcal{D}(g)||_2^2 = \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||x_i - g||_2^2 + ||g - p_\\mathcal{D}(g)||_2^2\n",
    "$$\n",
    "\n",
    "On obtient : \n",
    "\n",
    "$$\n",
    "f(\\mathcal{D}) = \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||x_i - g||_2^2 + ||g - p_\\mathcal{D}(g)||_2^2 - \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||p_\\mathcal{D}(x_i) - p_\\mathcal{D}(g)||_2^2\n",
    "$$\n",
    "\n",
    "Comme $||p_\\mathcal{D}(x_i) - p_\\mathcal{D}(g)||_2^2$ est une constante pour toute droite parallèle à $\\mathcal{D}$ on en déduit que la droite $\\mathcal{D}^\\star$ minimisant $f$ passe **nécessairement** par $g$ (dans ce cas $g = \\mathcal{D}^\\star(g)$). \n",
    "\n",
    "Si l'on se restreint aux droite passant par $g$, $f$ se récrit :\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "f(\\mathcal{D}) &= &\\sum_{1 \\leq i \\leq n} \\frac{1}{n}||x_i - g||_2^2 - \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||p_\\mathcal{D}(x_i) - p_\\mathcal{D}(g)||_2^2\\\\\n",
    "&=&\\sum_{1 \\leq i \\leq n} \\frac{1}{n}||x_i - g||_2^2 - \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||p_\\mathcal{D}(x_i - g)||_2^2\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> La droite minimisant les projections, **tout comme les droites de régressions** passe par le centre de gravité du nuage de points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La droite $\\mathcal{D}^\\star$ recherchée est donc aussi celle maximisant :\n",
    "\n",
    "$$\n",
    "g(\\mathcal{D}) = \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||p_\\mathcal{D}(x_i - g)||_2^2\n",
    "$$\n",
    "\n",
    "> $||x_i - g||_2$ est la ***longueur*** de la donnée $i$, on cherche à **maximiser** la longueur des projetés $||p_\\mathcal{D}(x_i - g)||_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information et inertie du nuage\n",
    "\n",
    "L'erreur produite en considérant les points $p_{\\mathcal{D}^\\star}(x_i)$ plutôt que les points initiaux $x_i$ vaut :\n",
    "\n",
    "$$\n",
    "f(\\mathcal{D}^\\star) = \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||x_i - g||_2^2 - \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||p_{\\mathcal{D}^\\star}(x_i) - p_{\\mathcal{D}^\\star}(g)||_2^2\n",
    "$$\n",
    "\n",
    "La quantité $I = \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||x_i - g||_2^2$ est appelée [***inertie du nuage***](https://fr.wikipedia.org/wiki/Inertie) et vaut la **variance** de celui-ci. En analyse des donnée cette quantité est synonyme de ***l'information*** contenue dans le nuage.\n",
    "\n",
    "La régression essaie d'expliquer une caractéristique par une (ou plusieurs) autres, la projection cherche à trouver un axe (une nouvelle caractéristique) combinant au mieux les caractéristiques initiales, c'est à dire conservant le maximum d'information.\n",
    "\n",
    "> La droite $\\mathcal{D}^\\star$ recherchée est celle qui **maximise l'information conservée**. On cherche **un axe** qui explique au mieux les $m$ axes (caractéristiques) initiaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrélations\n",
    "\n",
    "Supposons que l'on ait la droite $\\mathcal{D}^\\star$ maximisant l'information (inertie, variance) conservée du nuage. On sait que cette droite passe par le centre de gravité du nuage $g$ et considérons $\\overrightarrow{u}$ un vecteur directeur unitaire ($||\\overrightarrow{u}||_2 = 1$) de celle-ci.\n",
    "\n",
    "Ce vecteur est une combinaison linaire des caractéristiques initiales (les $x^j$, pour $1 \\leq j \\leq m$) et pour savoir comment il réagit par rapport à celles-ci on a coutume de considérer ses **corrélations linaires** avec celles ci.\n",
    "\n",
    "* si $r(\\overrightarrow{u}, x^j) \\simeq 1$ : \n",
    "  * les points $x_i$ se projetant dans le sens de $\\overrightarrow{u}$ auront tendance à avoir une valeur $x_i^j$ importante,\n",
    "  * les points $x_i$ se projetant dans le sens opposé de $\\overrightarrow{u}$ auront tendance à avoir une valeur $x_i^j$ faible,\n",
    "* si $r(\\overrightarrow{u}, x^j) \\simeq -1$ : \n",
    "  * les points $x_i$ se projetant dans le sens de $\\overrightarrow{u}$ auront tendance à avoir une valeur $x_i^j$ faible,\n",
    "  * les points $x_i$ se projetant dans le sens opposé de $\\overrightarrow{u}$ auront tendance à avoir une valeur $x_i^j$ importante,\n",
    "* si $r(\\overrightarrow{u}, x^j) \\simeq 0$ : la caractéristique $x^j$ n'influe pas linéairement sur $\\overrightarrow{u}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de l'axe\n",
    "\n",
    "On suppose à partir de maintenant que nos données **centrées**. Cela va rendre la résolution plus élégante, sans perte de généralité.\n",
    "\n",
    "\n",
    "D'après ce qui précède, on cherche à maximiser $g$ pour les droites $\\mathcal{D}$ passant par l'origine :\n",
    "\n",
    "$$\n",
    "g(\\mathcal{D}) = \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||p_\\mathcal{D}(x_i)||_2^2\n",
    "$$\n",
    "\n",
    "### ré-écriture matricielle du problème\n",
    "\n",
    "Les droites que l'on considère passant toutes par l'origine, chacune d'entre elle est entièrement déterminée par un vecteur directeur unitaire **colonne** $\\overrightarrow{u}$ à $m$ dimensions. On a :\n",
    "\n",
    "* $\\overrightarrow{u} = (u_1, \\dots, u_m)$\n",
    "* $||\\overrightarrow{u} ||_2 = 1\n",
    "\n",
    "De là, en utilisant la matrice de donnée $X$, on a : \n",
    "\n",
    "$$ X \\cdot \\overrightarrow{u} =\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "p_\\mathcal{D}(x_1)\\\\\n",
    "\\vdots\\\\\n",
    "p_\\mathcal{D}(x_i)\\\\\n",
    "\\vdots \\\\\n",
    "p_\\mathcal{D}(x_n)\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "et donc :\n",
    "\n",
    "$$\n",
    "g(\\mathcal{D}) = ^t(X\\overrightarrow{u})D(X\\overrightarrow{u}) = ^tu(^tXDX)\\overrightarrow{u}\n",
    "$$\n",
    "\n",
    "Avec $D = \\frac{1}{n} \\cdot I_n$ ($I_n$ est la matrice identité à $n$ lignes).\n",
    "\n",
    "On conclut ce calcul matriciel en remarquant que $(^tXDX)$ est égale à la ***matrice de variance $V$*** (matrice carrée symétrique à $m$ dimensions) :\n",
    "\n",
    "\n",
    "$$(^tXDX) = V = \\left(\n",
    "\\begin{array}{cccccc}\n",
    "\\text{cov}(x^1, x^1)&\\dots &\\text{cov}(x^1, x^j) &\\dots &\\text{cov}(x^1, x^m)\\\\\n",
    "     &      &\\vdots&      &  \\\\\n",
    "\\text{cov}(x^i, x^1)&\\dots &\\text{cov}(x^i, x^j)&\\dots &\\text{cov}(x^i, x^m)\\\\\n",
    "     &      &\\vdots&      & \\\\\n",
    "\\text{cov}(x^m, x^1)&\\dots &\\text{cov}(x^m, x^j)&\\dots &\\text{cov}(x^m, x^m)\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Notre problème revient ainsi à trouver un vecteur unitaire $\\overrightarrow{u}$ maximisant l'équation : \n",
    "$$^t\\overrightarrow{u}V\\overrightarrow{u}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résolution\n",
    "\n",
    "Trouver un vecteur unitaire $\\overrightarrow{u}^\\star$ maximisant l'équation : \n",
    "$$^t\\overrightarrow{u}V\\overrightarrow{u}$$\n",
    "\n",
    "On a donc :\n",
    "* Si $\\overrightarrow{u}$ est un vecteur propre unitaire de $V$ on a $Vu = \\lambda \\overrightarrow{u}$ et de là ${^t\\overrightarrow{u}} V u = \\lambda {^t \\overrightarrow{u}}u = \\lambda ||\\overrightarrow{u}||_2^2 = \\lambda$. On en déduit que $\\lambda \\geq 0$ pour tout vecteur propre.\n",
    "* $V$ est symétrique\n",
    "\n",
    "Les remarques précédentes montrent que $V$ est une **matrice semi-définie positive** : ses vecteurs propres unitaires forment une base orthonormée de $\\mathbb{R}^m$. Soient alors $(u_1, \\dots u_m)$ une base orthonormée de vecteurs propres de $V$ associés aux valeurs propres $\\lambda_1 \\geq \\dots \\geq \\lambda_m$. \n",
    "\n",
    "Tout vecteur unitaire $\\overrightarrow{u}$ peut donc s'écrire sous la forme :\n",
    "\n",
    "$$\n",
    "\\overrightarrow{u} = \\sum_{i=1}^m \\alpha_i u_i\n",
    "$$ \n",
    "\n",
    "De là :\n",
    "\n",
    "$$ {^t\\overrightarrow{u}} V \\overrightarrow{u}  = {^t(\\sum_{i=1}^p} \\alpha_i u_i) V (\\sum_{i=1}^p \\alpha_i u_i) = \\sum_{i, j} \\lambda_j \\alpha_i \\alpha_j {^tu_i} u_j = \\sum_{i} \\lambda_i \\alpha_i^2 \\leq \\lambda_1 \\sum_i \\alpha_i^2 = \\lambda_1$$\n",
    "\n",
    "Et comme :\n",
    "\n",
    "$$\n",
    "{^tu_1} V u_1 = \\lambda_1\n",
    "$$\n",
    "\n",
    "> La droite $\\mathcal{D}^\\star$ maximisant les projections est celle de vecteur directeur $u_1$ qui est le vecteur propre de $V$ associé à la plus grande de ses valeurs propres $\\lambda_1$. Cet axe est appelé ***axe factoriel***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valeurs des projetés\n",
    "\n",
    "La droite de projection $\\mathcal{D}^\\star$ est la droite :\n",
    "\n",
    "* passant par $0$ (car nos données sont centrée)\n",
    "* de vecteur directeur, le vecteur propre (vecteur colonne $u_1$) associé à la plus grande valeur propre ($\\lambda_1$) de la matrice de variance des données.\n",
    "\n",
    "Ceci s'écrit :\n",
    "\n",
    "$$\\left(\n",
    "\\begin{array}{c}\n",
    "p_{\\mathcal{D}^\\star}(x_1)\\\\\n",
    "\\vdots\\\\\n",
    "p_{\\mathcal{D}^\\star}(x_i)\\\\\n",
    "\\vdots\\\\\n",
    "p_{\\mathcal{D}^\\star}(x_n)\n",
    "\\end{array}\n",
    "\\right) =\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "x_1 \\cdot u_1\\\\\n",
    "\\vdots\\\\\n",
    "x_i \\cdot u_1 \\\\\n",
    "\\vdots\\\\\n",
    "x_n \\cdot u_1\n",
    "\\end{array}\n",
    "\\right) =\n",
    "\n",
    "Xu_1 = c\n",
    "$$\n",
    "\n",
    "> Les coordonnées $c = Xu_1$ des points sur l'axe principal sont nommées ***composantes principales***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de la projection\n",
    "\n",
    "L'axe factoriel $\\mathcal{D}^\\star$ passe par l'origine car nos données sont centrées et son vecteur directeur est le vecteur propre $u_1$ associé à la plus grande valeur propre $\\lambda_1$ de la matrice de variance des données.\n",
    "\n",
    "#### Information conservé\n",
    "\n",
    "On a :\n",
    "\n",
    "* **information du nuage** qui est égale à son inertie : $I = \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||x_i - g||_2^2 =  \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||x_i||_2^2$ (les données sont centrées)\n",
    "* **information de l'axe factoriel** qui est égale à l'inertie des projections : $I_{\\mathcal{D}^\\star} = \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||p_{\\mathcal{D}^\\star}(x_i) - p_{\\mathcal{D}^\\star}(g)||_2^2 =  \\sum_{1 \\leq i \\leq n} \\frac{1}{n}||p_{\\mathcal{D}^\\star}(x_i)||_2^2$ (car $\\mathcal{D}^\\star$ passe par $g$) et on a vu que $I_{\\mathcal{D}^\\star} = \\lambda_1$ la plus grande valeur propre de la matrice de variance.\n",
    "\n",
    "> Le ***pourcentage d'information véhiculée par l'axe principal'*** est égale à : ${I_{\\mathcal{D}^\\star}} / {I}$\n",
    "\n",
    "#### Interprétation de l'axe factoriel\n",
    "\n",
    "L'axe factoriel rend compte **des tendances** lourdes des données. Pour connaître lesquelles, on a coutume de regarder les corrélations des caractéristiques initiales avec la nouvelle caractéristique : $\\text{cor}(c, x^j)$, pour $1 \\leq j \\leq m$.\n",
    "\n",
    "Ces corrélation nous donnerons :\n",
    "\n",
    "* les caractéristiques qui ne donnent pas de tendances à l'axe : celle de corrélation proche de 0\n",
    "* les caractéristiques de tendances positive à l'axe (les coordonnée selon l'axe factoriel et selon la caractéristique vont dans le même sens) : celles de corrélation proche de 1\n",
    "* les caractéristiques de tendances négative à l'axe (les coordonnée selon l'axe factoriel et selon la caractéristique vont dans le sens opposé) : celles de corrélation proche de -1\n",
    "\n",
    "#### Qualité de la projection\n",
    "\n",
    "Si l'axe factoriel est celui qui maximise **globalement** les projections, il se peut d'individuellement, certaines données ne se représentent pas bien sur cet axe. \n",
    "\n",
    "> La ***qualité de la projection de $x_i$*** est calculée par : $||p_\\mathcal{D}(x_i)|| / ||x_i|| = ||x_i \\cdot u_1|| / ||x_i|| = \\cos(\\theta_i)$, avec $\\theta_i$ l'angle que fait \n",
    "\n",
    "Plus l'angle entre la donnée initiale et l'axe est importante, plus il se projettera vers l'origine, moins il faudra en tenir compte dans l'analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.display(display.Image(\"projection-données.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plus de 1 axe\n",
    "\n",
    "Soit $(u_1, \\dots u_m)$ une base orthonormée de vecteurs propres de la matrice de variance $V$ associés aux valeurs propres $\\lambda_1 \\geq \\dots \\geq \\lambda_m$. \n",
    "\n",
    "On peut montrer que chercher un sous espace vectoriel de dimension $k$ maximisant la projection des données correspond à l'espace formé des $k$ premiers vecteurs propres $(u_1, \\dots, u_k)$. L'information conservée par ce sous espace est $\\sum_{1 \\leq j \\leq k} \\lambda_j$\n",
    "\n",
    "> La base des vecteurs propres de la matrice de variance correspond à une base correspondant mieux aux données puisque chaque axe est proche\n",
    "\n",
    "Lorsque l'on analyse $k$ axes, on regarde donc les $k$ premiers vecteurs propres, car c'est ceux qui vont maximiser l'information conservée.\n",
    "\n",
    "Enfin, les représentations graphiques se font en choisissant deux axes (souvent $u_1 et $u_2$ mais pas toujours) qui permettent de visualiser les points, même si l'espace de départ est très grand $m >> 1$. On est garantit que les axes choisit sont significatifs pour les données, ce qui n'est pas le cas si l'on prend 2 caractéristiques initiales.\n",
    "\n",
    "### Vecteurs propres\n",
    "\n",
    "L'ensemble des vecteurs propres de la matrice de variance est noté $U$.C'est une matrice carrée à $m$ dimensions, où chaque colonne colonne correspond à un vecteur propre (la $j$ème colonne correspond à $u_j$) écrit dans la base initiale.\n",
    "\n",
    "La $j$ème colonne expliquera $\\lambda_j / I$ pourcent de l'information totale.\n",
    "\n",
    "### Composantes principales\n",
    "\n",
    "Les composantes principales correspondent aux coordonnées des points dans la base des vecteurs propres. Elle se note sous la forme matricielle par une une matrice $C$ à $m$ colonnes et $n$ lignes telle que :\n",
    "\n",
    "$$C = XU$$\n",
    "\n",
    "La $j$ème colonne de $C$, notée $c^j$, correspond aux coordonnées des $n$ point selon le vecteur propre $u_j$ et la $i$ème ligne, notée $c_i$ correspond aux valeurs du $i$ème point dans la nouvelle base.\n",
    "\n",
    "### Qualité de la projection\n",
    "\n",
    "La qualité de la projection d'un point $x_i$ selon l'axe $u_j$ s'exprime avec le $\\cos^2(\\theta_i^j)$ de l'angle $\\theta_i^j$ qu'il fait avec cet axe. :\n",
    "\n",
    "$$\\cos^2(\\theta^j_i) = \\frac{(c_i^j)^2}{\\frac{1}{n}(\\sum_{1\\leq k \\leq m} (x_i^k)^2)}$$\n",
    "\n",
    "> On utilise les $\\cos^2$ pour mesurer la qualité de la projection car ils s'additionnent : Le $\\cos^2$ de l'angle que fait le point $x_i$ avec les axes $u_j$ et $u_k$ vaut $\\cos^2(\\theta^j_i) + \\cos^2(\\theta^k_i)$\n",
    "\n",
    "### Corrélations\n",
    "\n",
    "La corrélation de l'axe $u_j$ par rapport aux anciennes caractéristiques $x^i$ ($1 \\leq i \\leq m$) s'écrit : \n",
    "\n",
    "$$\\text(corr)(x^i, c^j)$$\n",
    "\n",
    "Lorsque l'on considère deux axes $u_j$ et $u_k$, on peut montrer que  : \n",
    "\n",
    "$$0 \\leq \\text{cor}^2(x^i, c^j) + \\text{cor}^2(x^i, c^k) \\leq 1$$\n",
    "\n",
    "Les points $(\\text{cor}^2(x^i, c^j), \\text{cor}^2(x^i, c^k))$ pour $1 \\leq i \\leq m$ sont donc tous dans le disque unité. On appelle l'ensemble de ces points le ***cercle des corrélations pour les axes $j$ et $k$***. Il montre dans une figure synthétique les corrélations entre les anciennes caractéristiques et deux nouvelles ($j$ et $k$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Données centrées et réduites\n",
    "\n",
    "Lorsque les données sont centrées et réduites l'information de chaque axe se calcule plus aisément.\n",
    "\n",
    "Si les **données sont centrées et réduites**, on a $||x^j||_2 = 1$ pour tout $1 \\leq j \\leq m$ et donc :\n",
    "\n",
    "* chaque caractéristique initiale véhicule une information (inertie) de 1\n",
    "* l'information totale du nuage est $I = m$\n",
    "* le pourcentage d'information véhiculé par chaque nouvel axe vaut $\\frac{\\lambda_1}{m}$\n",
    "\n",
    "> Dans une optique d'**Analyse des données** nos projections se feront **toujours** sur des données centrées et réduites.\n",
    "\n",
    "## Exemple de l'épreuve\n",
    "\n",
    "On reprend nos données d'épreuve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des données $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "épreuve = pandas.read_csv(\"./épreuve.txt\", delim_whitespace=True)\n",
    "épreuve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On centre et on réduit nos données et on appelle nos données $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = épreuve   # nom du jeu de données\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = pandas.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informations du nuage\n",
    "\n",
    "On vérifie que l'information totale de notre nuage vaut 2 et que chaque axe véhicule une information de 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inertie du nuage\n",
    "\n",
    "> **Attention** : `X ** 2`est le produit terme à term , pas la multiplication de matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = sum((X ** 2).sum(axis=1)) / len(X.index)\n",
    "\n",
    "I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pour chaque axe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour chaque axe\n",
    "\n",
    "(X ** 2).sum(axis=0) / len(X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Axe quelconque\n",
    "\n",
    "On peut essayer de calculer l'inertie d'un axe quelconque.\n",
    "\n",
    "\n",
    "Commençons par trouver un axe unitaire quelconque :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform\n",
    "from math import pi, cos, sin\n",
    "import numpy as np\n",
    "\n",
    "theta = uniform(0, pi)\n",
    "u = np.array([cos(theta), sin(theta)])\n",
    "\n",
    "u "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'inertie associé à cet axe est :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/len(X) * sum((X @ u) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et son pourcentage d'inertie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1/len(X) * sum((X @ u) ** 2)) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordonnées de chaque individu pour chaque axe\n",
    "\n",
    "Il suffit de faire le produit scalaire de $X$ avec un vecteur directeur de l'axe. \n",
    "\n",
    "Nos caractéristiques initiales sont $e_1 = (1, 0)$ et $e_2 = (0, 1)$.\n",
    "\n",
    "> En pandas, le [produit matriciel](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dot.html) est soit la méthode [`pandas.DataFrame.dot`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dot.html) soit l'opérateur `@`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dot((1, 0)) # équivalent à X @ (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X @ (0, 1) # équivalent à X.dot((0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longueur des individus\n",
    "\n",
    "> **Attention !** Ce n'est pas parce que nos données sont centrées et réduites que la longueur de chaque individu vaut 1. Ce sont les caractéristiques qui sont de moyennes nulle et de variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normes = (X ** 2).sum(axis=1)  / len(X.columns)\n",
    "\n",
    "normes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualité de la projection\n",
    "\n",
    "On regarde le $\\cos^2$ de l'angle pour chaque chaque individu et pour chaque axe. \n",
    "\n",
    "> On utilise les $\\cos^2$ car ils s'additionnent si l'on cherche la projection sur un plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X ** 2).div(normes, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La somme des $\\cos^2$ fait bien 1 pour chaque individu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de corrélation\n",
    "\n",
    "On va procéder à la recherche des axes factoriels, ce qui revient à chercher les vecteur propres de la matrice de variance. \n",
    "\n",
    "> Dans le cas de données **centrées et réduites**, la matrice de variance est égale à la matrice des corrélations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "D = 1/len(X.index) * np.identity(len(X.index))\n",
    "\n",
    "X.transpose() @ D @ X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résolution des projections\n",
    "\n",
    "Le code ci-dessous crée toutes les données nécessaires pour utiliser les axes factoriels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "pca = PCA() \n",
    "pca.fit(X)\n",
    "\n",
    "U = np.transpose(pca.components_) # vecteurs propres\n",
    "I = pandas.DataFrame(np.transpose(pca.explained_variance_ratio_), columns=[\"pourcentage\"])  # information véhiculée\n",
    "\n",
    "C = pandas.DataFrame(X @ U, index=X.index) # nouvelles coordonnées\n",
    "\n",
    "corrélations = pandas.DataFrame([[C[facteur].corr(X[column]) for facteur in C] for column in X], index=X.columns)\n",
    "cos2 = (C**2).div((X**2).sum(axis=1), axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrélations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information expliquée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le premier axe (axe 0) explique prêt de 90% de l'information du nuage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Représentation graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessous est générique, on change les variables `axe_x` et `axe_y` avec le numéros des axes factoriels à représenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axe_x = 0\n",
    "axe_y = 1\n",
    "\n",
    "couleurs = [sns.color_palette()[0]] * len(C) # une couleur pour chaque donnée\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7)) \n",
    "\n",
    "draw = C.plot.scatter(axe_x, axe_y, color=couleurs, ax=ax)\n",
    "\n",
    "for index, row in C.iterrows():\n",
    "    draw.annotate(str(index), (row[axe_x], row[axe_y]))\n",
    "    \n",
    "plt.axvline(0)\n",
    "plt.axhline(0)\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cercle des corrélations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axe_x = 0\n",
    "axe_y = 1\n",
    "\n",
    "couleurs = [sns.color_palette()[3]] * len(corrélations)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7)) \n",
    "\n",
    "draw = corrélations.plot.scatter(x=axe_x, y=axe_y, ax=ax, \n",
    "                                 color=couleurs)\n",
    "\n",
    "\n",
    "for index, row in corrélations.iterrows():\n",
    "    draw.annotate(str(index), (row[axe_x], row[axe_y]))\n",
    "\n",
    "draw.add_patch(plt.Circle((0, 0), radius=1, color=(0, 0, 0, .3)))\n",
    "\n",
    "plt.xlim(-1.2, 1.2)\n",
    "plt.ylim(-1.2, 1.2)\n",
    "plt.axvline(0)\n",
    "plt.axhline(0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L'axe 0 (horizontal) est très corrélé au temps et aux erreurs\n",
    "* L'axe 1 (vertical) oppose le temps et les erreurs\n",
    "\n",
    "Le 1er axe est un axe d'**aptitude** : les élèves se projetant à droite auront pris beaucoup de temps et fait beaucoup d'erreurs, ceux se projetant à gauche auront pris peu de temps et fait peut d'erreurs. Le second axe est un axe d'**attitude**. Les élèves se projetant en haut auront bâclé leur contrôle en terminant rapidement au détriment de l'exactitude, les élèves se projetant en bas auront  pris leur temps pour faire peu d'erreurs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualité de la représentation\n",
    "\n",
    "On montre les $\\cos^2$ des différentes données aux 2 axes factoriels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on a que 2 axes, la somme des colonnes pour chaque ligne fait 1 : un point bien projeté sur un axe signifie mal projeté sur l'autre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les élèves les plus marqué par le second axe sont ceux ne se représentant pas bien sur le 1er. C'est les élèves 1 et 9 essentiellement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
