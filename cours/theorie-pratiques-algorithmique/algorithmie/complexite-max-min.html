<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Complexité max/min | cours d’informatique</title>
<meta name="generator" content="Jekyll v4.2.0">
<meta property="og:title" content="Complexité max/min">
<meta name="author" content="François Brucker">
<meta property="og:locale" content="en_US">
<meta name="description" content="Support de cours/td d’informatique à l’école centrale marseille.">
<meta property="og:description" content="Support de cours/td d’informatique à l’école centrale marseille.">
<link rel="canonical" href="/cours_informatique/cours/theorie-pratiques-algorithmique/algorithmie/complexite-max-min.html">
<meta property="og:url" content="/cours_informatique/cours/theorie-pratiques-algorithmique/algorithmie/complexite-max-min.html">
<meta property="og:site_name" content="cours d’informatique">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Complexité max/min">
<script type="application/ld+json">
{"description":"Support de cours/td d’informatique à l’école centrale marseille.","headline":"Complexité max/min","@type":"WebPage","url":"/cours_informatique/cours/theorie-pratiques-algorithmique/algorithmie/complexite-max-min.html","author":{"@type":"Person","name":"François Brucker"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/cours_informatique/assets/main.css">

  <link rel="stylesheet" href="/cours_informatique/assets/custom.css">
<link type="application/atom+xml" rel="alternate" href="/cours_informatique/feed.xml" title="cours d'informatique">
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
<header class="site-header" role="banner">

  <div class="wrapper">
<a class="site-title" rel="author" href="/cours_informatique/">cours d'informatique</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger">
            <a class="page-link" href="/cours_informatique/about"> about </a>
        </div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Complexité max/min</h1>Auteur : <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">François Brucker</span></span>
  </header>

  <div class="post-content">
    <blockquote class="chemin">
  <p><a href="/cours_informatique/cours/theorie-pratiques-algorithmique/">Théorie et pratiques algorithmique</a> / <a href="/cours_informatique/cours/theorie-pratiques-algorithmique/algorithmie/">algorithmie</a> / <a href="/cours_informatique/cours/theorie-pratiques-algorithmique/algorithmie/complexite-max-min.html">complexité max/min</a></p>

  <p>prérequis :</p>

  <ul>
    <li><a href="/cours_informatique/cours/theorie-pratiques-algorithmique/algorithmie/pseudo-code.html">algorithmie/pseudo-code</a></li>
  </ul>
</blockquote>

<p>Où l’on se donne des outils pour mesurer (théoriquement et en pratique) les performances d’un algorithmes</p>

<h2 id="mesures-en-mathcalo">mesures en $\mathcal{O}$</h2>

<p>Mesurer les performances d’un algorithme se fera presque exclusivement en utilisant des $\mathcal{O}$ (<em>grand O</em>)</p>

<blockquote class="note">
  <p>Une fonction $f(N)$ est en $\mathcal{O}(f’(N))$ s’il existe 2 constantes $c_0$ et $N_0$ tels que $f(N) &lt; c_0 \cdot f’(N)$ pour tout $N &gt; N_0$.</p>
</blockquote>

<p>Cela permet :</p>

<ul>
  <li>d’avoir un majorant de notre mesure lorsque $N$ devient grand</li>
  <li>de ne pas s’occuper des constantes puisque (on va le démontrer) une fonction en $\mathcal{O}(\mbox{constante})$ est également en $\mathcal{O}(1)$</li>
  <li>de ne pas s’occuper de la proportionnalité car (on va le démontrer) une fonction en $\mathcal{O}(\mbox{constante} \cdot f(N))$ est également en $\mathcal{O}(f(N))$</li>
</ul>

<blockquote class="note">
  <p>Connaitre le comportement en $\mathcal{O}$ d’une mesure dépendant de $N$ nous donne un majorant de son comportement lorsque $N$ devient grand. Si le majorant n’est pas trop éloigné de la mesure originale, cela nous donne une <strong>idée générale</strong> de la valeur de la mesure lorsque $N$ devient grand.</p>
</blockquote>

<p>Ceci est plutôt intéressant en algorithmie car l’on ne connait pas toujours exactement le nombre d’opérations élémentaires utilisées, mais on peut les majorer de façon assez précise. On utilisera ainsi les $\mathcal{O}$ pour mesurer :</p>

<ul>
  <li>le nombre d’opérations élémentaires effectuée par l’algorithme avant de s’arrêter</li>
  <li>le temps mis par l’algorithme pour s’exécuter</li>
  <li>la taille de la mémoire utilisée pour par l’algorithme</li>
</ul>

<p>Par rapport à la taille $N$ de l’entrée de l’algorithme.</p>

<h3 id="arithmétique-des-mathcalo">arithmétique des $\mathcal{O}$</h3>

<p>Par abus de langage, on notera :</p>

<ul>
  <li>$\mathcal{O}(f(N))$ plutôt que soit $f’(N)$ une fonction en $\mathcal{O}(f(N))$</li>
  <li>$f(N) = \mathcal{O}(g(N))$ plutôt que : “la fonction $f(N)$ est en $\mathcal{O}(g(N))$”</li>
  <li>$\mathcal{O}(f(N)) \Rightarrow \mathcal{O}(g(N))$ plutôt que “une fonction en $\mathcal{O}(f(N))$ est également en $\mathcal{O}(g(N))$”</li>
  <li>$\mathcal{O}(f(N)) \Leftrightarrow \mathcal{O}(g(N))$ plutôt que “une fonction en $\mathcal{O}(f(N))$ est également en $\mathcal{O}(g(N))$ et réciproquement”</li>
</ul>

<blockquote class="note">
  <p>On a les règles suivantes :</p>

  <ol>
    <li>$\mathcal{O}(A) \Leftrightarrow \mathcal{O}(1)$, avec $A$ une contante strictement positive</li>
    <li>$\mathcal{O}(N^p) \Rightarrow \mathcal{O}(N^q)$ pour $q \geq p$</li>
    <li>$f(N) = \mathcal{O}(g(N))$ implique $\mathcal{O}(f(N) + g(N) + h(N)) \Rightarrow \mathcal{O}(g(N) + h(N))$</li>
    <li>$f(N) = \mathcal{O}(g(N))$ implique $\mathcal{O}(f(N) \cdot g(N) \cdot h(N) + h’(N)) \Rightarrow \mathcal{O}((g(N))^2 \cdot h(N)+ h’(N))$</li>
  </ol>

  <p>Et en combinant les $\mathcal{O}$ pour $f$ et $g$ deux fonction positives :</p>

  <ul>
    <li>$\mathcal{O}(f(N)) + \mathcal{O}(g(N)) \Rightarrow \mathcal{O}(f(N) + g(N))$</li>
    <li>$\mathcal{O}(f(N)) \cdot \mathcal{O}(g(N)) \Rightarrow \mathcal{O}(f(N) \cdot g(N))$</li>
  </ul>

</blockquote>

<blockquote class="a-faire">
  <p>Démontrez ces propriétés.</p>
</blockquote>

<details><summary>Démonstration de $\mathcal{O}(A) \Leftrightarrow \mathcal{O}(1)$, avec $A$ une contante strictement positive</summary><div>
<p>Soit $f(N) = \mathcal{O}(A)$. Il existe donc $c_0$ et $N_0$ tels que pour tout $N &gt; N_0$, on ait $f(N) &lt; c_0 \cdot A$. En posant $c’_0 = c_0 \cdot A$, on a $f(N) &lt; c’_0 \cdot 1$ pour tout $N &gt; N_0$ donc $f(N) = \mathcal{O}(1)$.</p>

<p>Réciproquement, soit $f(N) = \mathcal{O}(1)$. Il existe donc $c_0$ et $N_0$ tels que pour tout $N &gt; N_0$, on ait $f(N) &lt; c_0 \cdot 1$. En posant $c’_0 = c_0 / A$, on a $f(N) &lt; c’_0 \cdot A$ pour tout $N &gt; N_0$ donc $f(N) = \mathcal{O}(A)$.</p>

</div></details>

<details><summary>Démonstration de $\mathcal{O}(N^p) \Rightarrow \mathcal{O}(N^q)$ pour $q \geq p$</summary><div>
<p>Soit $f(N) = \mathcal{O}(N^p)$. Il existe donc $c_0$ et $N_0$ tels que $f(N) &lt; c_0 \cdot N^p$ pour $N &gt; N_0$.
Comme $1 &lt; 2 \cdot N^\alpha$ pour $\alpha \geq 0$ et $N&gt; 1$, on a $N^p &lt; c_0 \cdot N^q$ pour $c_0 = 2$, $N &gt; 1 = N_0$  et $p \leq q$ : $N^p = \mathcal{O}(N^q)$ pour tout $p \leq q$</p>

</div></details>

<details><summary>Démonstration de $f(N) = \mathcal{O}(g(N))$ implique $\mathcal{O}(f(N) + g(N) + h(N)) \Rightarrow \mathcal{O}(g(N) + h(N))$</summary><div>
<p>Soit $f(N) = \mathcal{O}(g(N))$. Il existe donc $c_0$ et $N_0$ tels que $f(N) &lt; c_0 \cdot N^p$ pour $N &gt; N_0$. Si $f’(N) = \mathcal{O}(f(N) + g(N) + h(N))$ il existe $c’_0$ et $N’_0$ tels que $f’(N) &lt; c’_0(f(N) + g(N) + h(N))$ pour $N &gt; N_0$.</p>

<p>De là, $f’(N) &lt; c’_0 c_0 g(N) + c’_0 g(N) + c’_0 h(N)$ pour $N &gt; \max \{ N_0, N’_0 \}$ ce qui implique $f’(N) &lt; \max \{ c’_0, c_0 \}^2 (g(N) + h(N))$ pour $N &gt; \max \{ N_0, N’_0 \}$ : $f’(N) = \mathcal{O}(g(N) + h(N))$</p>

</div></details>

<details><summary>Démonstration de $f(N) = \mathcal{O}(g(N))$ implique $\mathcal{O}(f(N) \cdot g(N) \cdot h(N) + h’(N)) \Rightarrow \mathcal{O}((g(N))^2 \cdot h(N)+ h’(N))$</summary><div>
<p>Soit $f(N) = \mathcal{O}(g(N))$. Il existe donc $c_0$ et $N_0$ tels que $f(N) &lt; c_0 \cdot N^p$ pour $N &gt; N_0$. Si $f’(N) = \mathcal{O}(f(N)\cdot g(N) \cdot h(N) + h’(N))$ il existe $c’_0$ et $N’_0$ tels que $f’(N) &lt; c’_0(f(N) \cdot g(N) \cdot h(N) + h’(N))$ pour $N &gt; N_0$.</p>

<p>De là, $f’(N) &lt; c’_0 (c_0 g(N) \cdot g(N) \cdot h(N) + h’(N)$ pour $N &gt; \max \{ N_0, N’_0 \}$ ce qui implique $f’(N) &lt; \max \{ c’_0, c_0 \}^2 (g(N)^2 \cdot  h(N) + h’(N))$ pour $N &gt; \max \{ N_0, N’_0 \}$ : $f’(N) = \mathcal{O}((g(N))^2 \cdot h(N) + h’(N))$</p>

</div></details>

<details><summary>Démonstration de  $\mathcal{O}(f(N)) + \mathcal{O}(g(N)) \Rightarrow \mathcal{O}(f(N) + g(N))$</summary><div>
<p>Soient $f’(N) = \mathcal{O}(f(N))$ et $g’ = \mathcal{O}(g(N))$, il existe donc $c_0$, $c’_0$, $N_0$ et $N’_0$ tels que $f’(N) &lt; c_0 f(N)$ pour $N &gt; N_0$ et $g’(N) &lt; c’_0 g(N)$ pour $N &gt; N’_0$.</p>

<p>On a alors $f’(N) + g’(N) &lt; \max \{c_0, c’_0\} (f(N) + g(N))$ pour $N &gt; \max \{ N_0, N’_0\}$ : $f’(N) + g’(N) = \mathcal{O}(f(N) + g(N))$.</p>

</div></details>

<details><summary>$\mathcal{O}(f(N)) \cdot \mathcal{O}(g(N)) \Rightarrow \mathcal{O}(f(N) \cdot g(N))$</summary><div>
<p>Soient $f’(N) = \mathcal{O}(f(N))$ et $g’ = \mathcal{O}(g(N))$, il existe donc $c_0$, $c’_0$, $N_0$ et $N’_0$ tels que $f’(N) &lt; c_0 f(N)$ pour $N &gt; N_0$ et $g’(N) &lt; c’_0 g(N)$ pour $N &gt; N’_0$.</p>

<p>On a alors $f’(N) \cdot g’(N) &lt; \max \{c_0, c’_0, 1 \}^2 (f(N) \cdot g(N))$ pour $N &gt; \max \{ N_0, N’_0\}$ car $f$ et $g$ sont positives : $f’(N) \cdot g’(N) = \mathcal{O}(f(N) \cdot g(N))$.</p>

</div></details>

<h3 id="conséquences-algorithmique">conséquences algorithmique</h3>

<p>La règle (1) montre qu’un nombre constant est toujours en $\mathcal{O}(1)$. Pour un algorithme, il est souvent compliqué de savoir exactement de combien d’<a href="/cours_informatique/cours/theorie-pratiques-algorithmique/algorithmie/pseudo-code.html#instruction-basique">opérations basique</a> est constitué une opération, ou le temps exact qu’elle va prendre (pour un ordinateur, cela dépend du type de processeur par exemple. L’addition avec un x68 est faites <a href="https://ensiwiki.ensimag.fr/index.php?title=Constructions_de_base_en_assembleur_x86">avec des registres</a> par exemple, et donc l’addition nécessite 2 opération du processeur). Mais on pourra toujours montrer qu’il y en a un nombre constant (ou borné par un nombre constant) :</p>

<blockquote class="note">
  <p>La complexité d’une opération basique nécessite $\mathcal{O}(1)$ opérations.</p>
</blockquote>

<p>De là :</p>

<blockquote class="note">
  <p>un nombre constant d’opérations basiques nécessite $\mathcal{O}(1)$ opérations.</p>
</blockquote>

<p>Les règles précédentes permettent plus généralement de montrer :</p>

<blockquote class="note">
  <p>$\mathcal{O}(A \cdot f(N)) \Leftrightarrow A \cdot \mathcal{O}(f(N)) \Leftrightarrow \mathcal{O}(f(N))$, avec $A$ une contante strictement positive et $f(N)$ une fonction strictement positive pour $N &gt; N_0$</p>
</blockquote>

<p>Ceci est pratique, car cela permet de ne pas compter toutes les opérations basiques précisément. Ainsi, en reprenant l’exemple de la partie <a href="/cours_informatique/cours/theorie-pratiques-algorithmique/algorithmie/pseudo-code.html#complexit%C3%A9">complexité des pseudo-code</a> :</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x = 30
if ((x &gt; 12) AND (y &lt; 36)):
    z = x * "coucou"
</code></pre></div></div>

<ol>
  <li>on affecte un objet à x : 1 instruction, donc $\mathcal{O}(1)$ opérations.</li>
  <li>un test avec 2 comparaisons et un <code class="language-plaintext highlighter-rouge">AND</code> : 3 instructions, donc $\mathcal{O}(3) = \mathcal{O}(1)$ opérations.</li>
  <li>on affecte le résultat d’une opération élémentaire : 2 instructions, donc $\mathcal{O}(2) = \mathcal{O}(1)$ opérations.</li>
</ol>

<p>Un nombre total d’instructions de $3 \mathcal{O}(1) = \mathcal{O}(1)$ opérations.</p>

<p>En revanche, faites attention, cela ne marque que pour les constantes !</p>

<blockquote class="attention">
  <p>Si le nombre d’opérations élémentaires est variable on a : $n \cdot \mathcal{O}(1) = \mathcal{O}(n)$. On ne peut pas simplifier les variables.</p>
</blockquote>

<p>Enfin, comme en algorithmie on manipulera souvent des polynômes, on peut montrer facilement avec les règles précédentes que :</p>

<blockquote class="note">
\[\sum_{i=0}^na_i x^i = \mathcal{O}(x^n)\]
</blockquote>

<h2 id="complexité-dun-algorithme">complexité d’un algorithme</h2>

<p>On l’a vu dans la partie <a href="/cours_informatique/cours/theorie-pratiques-algorithmique/algorithmie/pseudo-code.html#complexit%C3%A9">pseudo-code</a>, la complexité est le nombre d’opérations basiques effectuées par un algorithme. Le nombre d’opérations basiques effectué par un pseudo-code va être dépendant des entrées de celui-ci, même si les entrées ont la même taille (on verra des exemples de ça).</p>

<p>On distinguera trois types de complexités :</p>

<ul>
  <li>nombre d’opérations basiques effectuées</li>
  <li>temps d’exécution d’un programme</li>
  <li>taille mémoire consommée pendant l’exécution</li>
</ul>

<p>Les complexités vont toutes dépendre des entrées, plus précisément d’un paramètre rendant compte de leur <strong>taille</strong>, c’est à dire du nombre de cases mémoires nécessaires pour les stocker.</p>

<blockquote class="attention">
  <p>Lorsque l’on donne des complexité c’est toujours en fonction d’un ou plusieurs paramètres qu’il <strong>faut</strong> expliciter</p>
</blockquote>

<h3 id="nombre-dopérations-basiques">nombre d’opérations basiques</h3>

<blockquote class="note">
  <p>La <strong>complexité</strong> (aussi parfois appelée <strong>complexité maximale</strong>) d’un algorithme est le <strong>nombre maximum d’opérations basiques</strong> effectué par celui-ci pour des entrées <strong>de taille totale donnée</strong>. Elle sera donnée en $\mathcal{O}(f(N))$, où $N$ est une variable rendant compte de la taille des données.</p>
</blockquote>

<p>La <strong>taille</strong> d’une entrée est proportionnelle au nombre de cases mémoires que celle-ci nécessite.</p>

<blockquote>
  <p>Lorsque vous entendrez parler de <em>complexité</em> d’un algorithme, ce sera par défaut <strong>toujours</strong> la complexité maximale.</p>
</blockquote>

<p>Il arrive que certains algorithmes aient un comportement très différent selon les entrées. Parler seulement de la complexité (nombre maximum d’opérations) ne permet pas alors de le caractériser complètement. On parlera alors aussi de :</p>

<blockquote class="note">
  <p>La <strong>complexité minimale</strong> d’un algorithme est le <strong>nombre minium d’opérations basiques</strong> effectué par celui-ci pour des entrées <strong>de taille totale donnée</strong>. Elle sera donnée en $\mathcal{O}(f(N))$, où $N$ est une variable rendant compte de la taille des données.</p>
</blockquote>

<p>Lorsque l’on calcule une complexité (maximale ou minimale) sous la forme d’un $\mathcal{O}(f(N))$, on tentera bien sur de trouver la fonction $f(N)$ la plus petite possible.</p>

<h3 id="temps-dexécution">temps d’exécution</h3>

<p>Un moyen efficace de mesurer la complexité d’un algorithme écrit sous la forme d’un code exécutable est de mesurer le temps mis pour son exécution pour un jeu d’entrée donné.</p>

<blockquote class="note">
  <p>la <strong>complexité en temps</strong> d’un algorithme est le temps mis pour l’exécuter en utilisant un jeu de donné <strong>pour lequel la complexité (max) est atteinte</strong> et d’une taille totale donnée.</p>
</blockquote>

<p>Le temps pris sera bien sur différent si l’on prend une machine plus puissante ou si l’on change le code de l’algorithme mais <strong>l’évolution de la complexité en temps par rapport à la taille des données est toujours proportionnelle à la complexité</strong>. Pour le voir, il suffit de mesurer la durée d’exécution de chaque instruction basique et de la borner par le max.</p>

<blockquote class="attention">
  <p>Si vous ne prenez pas un jeu de donné pour lequel la complexité de l’algorithme est atteinte, vous ne mesurez <strong>pas</strong> la complexité temporelle de l’algorithme…</p>
</blockquote>

<h3 id="taille-mémoire">taille mémoire</h3>

<blockquote class="note">
  <p>la <strong>complexité en espace</strong> d’un algorithme est le nombre maximum de cases mémoires utilisées pour l’exécuter en utilisant un jeu de donné de taille donnée.</p>
</blockquote>

<p>Comme la complexité, on la mesurera avec des $\mathcal{O}$.</p>

<p>Notez que la complexité en espace n’est pas forcément atteinte pour un jeu de donné donnant la complexité de l’algorithme, mais <strong>la complexité en espace sera toujours plus faible que la complexité</strong> (visiter une case mémoire nécessitant une opération élémentaire).</p>

<h3 id="complexité-de-méthodes-ou-de-structures">complexité de méthodes ou de structures</h3>

<p>Lorsque l’on code un algorithme, on a coutume (et c’est très bien) d’utiliser des fonctions, des méthodes ou des structures que l’on a pas écrite. Il faut en revanche bien connaître leurs complexités pour ne pas commettre d’erreur de calcul.</p>

<blockquote class="note">
  <p>Lorsque l’on calcul une complexité toutes les méthodes et fonctions doivent être examinées</p>
</blockquote>

<h4 id="complexité-de-structure">complexité de structure</h4>

<p>En informatique, les <strong>objets que l’on manipule ont des types</strong>. On connait déjà des <a href="/cours_informatique/cours/theorie-pratiques-algorithmique/algorithmie/pseudo-code.html#objets-basique">objets basiques</a> que sont de types booléens, entiers, réels ou encore chaines de caractères pour les quels toutes les opérations que l’on peut effectuer avec eux sont en $\mathcal{O}(1)$. Ce n’est plus le cas lorsque l’on utilise des type plus complexes, composé de types basiques comme les conteneurs comme les tableaux, ou encore les listes de python. Pour pouvoir calculer la complexité d’un algorithme les utilisant, il faut connaitre les complexités de ses opérations. Souvent, les opérations suivantes suffisent :</p>

<blockquote class="note">
  <p>Pour chaque type de donnée, il faut connaitre la complexité de :</p>

  <ul>
    <li>la création d’un objet de ce type</li>
    <li>la suppression d’un objet de ce type</li>
    <li>chaque méthode liée au type</li>
  </ul>

</blockquote>

<p>Prenons le type <a href="https://fr.wikipedia.org/wiki/Tableau_(structure_de_donn%C3%A9es)">tableau</a> comme exemple. Un tableau est un conteneur pouvant contenir $n$ objets (on appelle $n$ la taille d’un tableau). On peut accéder et affecter un objet au tableau grâce à un indice allant de $0$ à $n-1$ : si <code class="language-plaintext highlighter-rouge">t</code> est un tableau <code class="language-plaintext highlighter-rouge">t[i]</code> correspond à l’objet d’indice $i$ du tableau. Avec un tableau on peut :</p>

<ul>
  <li>créer un tableau de taille $n$ en $\mathcal{O}(1)$ opérations</li>
  <li>supprimer un tableau est possible en $\mathcal{O}(1)$ opérations</li>
  <li>récupérer et affecter l’objet d’indice $i$ du tableau (objet <code class="language-plaintext highlighter-rouge">t[i]</code>) se fait en $\mathcal{O}(1)$ opérations</li>
  <li>pour augmenter la taille d’un tableau, il faut recréer un tableau vide avec la nouvelle taille puis recopier tous les éléments de l’ancien tableau au nouveau. Cela se fait donc en $\mathcal{O}(n)$ opérations où $n$ est la taille de l’ancien tableau.</li>
  <li>pour réduire la taille d’un tableau, il faut recréer un tableau vide avec la nouvelle taille puis recopier les éléments que l’on veut garder de l’ancien tableau au nouveau. Cela se fait en $\mathcal{O}(n)$ opérations où $n$ est la taille du nouveau tableau.</li>
</ul>

<blockquote>
  <p>De façon pratique, un tableau est un ensemble des $n$ cases mémoires continues. Ce qui fait qu’on peut donc facilement les réserver et les libérer en une fois et que à la case mémoire d’indice $i$ vaut <code class="language-plaintext highlighter-rouge">&amp;t + i</code> où <code class="language-plaintext highlighter-rouge">&amp;t</code> est le numéro de la case mémoire d’indice $0$ du tableau.</p>
</blockquote>

<p>Le langage python ne connait pas les tableaux. Il utiliser le type <strong>liste</strong> à la place. Une liste peut être vue comme l’évolution du type tableau. On donne ici juste les complexités de cette structure pour que vous puissiez les utiliser dans vos programmes, nous ne les démontrerons pas :</p>

<ul>
  <li>créer et supprimer une liste de taille $n$ en $\mathcal{O}(1)$ opérations</li>
  <li>récupérer et affecter l’objet d’indice $i$ d’une liste (objet <code class="language-plaintext highlighter-rouge">t[i]</code>) se fait en $\mathcal{O}(1)$ opérations</li>
  <li>pour augmenter la taille d’une liste d’un élément se fait en $\mathcal{O}(1)$ opérations</li>
  <li>supprimer le dernier élément d’une liste se fait en $\mathcal{O}(1)$ opérations</li>
</ul>

<p>Une liste peut être vue comme un tableau dont on peut augmenter ou diminuer la taille par la fin en $\mathcal{O}(1)$ opérations.</p>

<blockquote class="attention">
  <p>Ne confondez pas liste et <a href="https://fr.wikipedia.org/wiki/Liste_cha%C3%AEn%C3%A9e">liste chaînée</a> ce n’est pas du tout la même structure !</p>
</blockquote>

<h4 id="fonction-et-méthodes-données">fonction et méthodes données</h4>

<p>Il faut connaître les différentes complexités des méthodes et fonctions utilisées. Ne vous laissez pas méprendre. Ce n’est pas parce qu’elle font 1 seule ligne que leur complexité est en $\mathcal{O}(1)$. Par exemple la complexité de la méthode <code class="language-plaintext highlighter-rouge">max</code> de python, qui prend en entrée une liste <code class="language-plaintext highlighter-rouge">l</code> :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">l</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">l</span><span class="p">.</span><span class="nb">max</span><span class="p">())</span>
</code></pre></div></div>

<p>Est de complexité $\mathcal{O}(n)$  où $n$ est la taille da liste <code class="language-plaintext highlighter-rouge">l</code> et pas $\mathcal{O}(1)$. Il <strong>faut</strong> en effet parcourir tous les éléments d’une liste (a priori non triée) pour en trouver le maximum.</p>

<h3 id="exemple">exemple</h3>

<p>Prenons par exemple l’algorithme écrit en python suivant :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">est_dans_tableau</span><span class="p">(</span><span class="n">valeur</span><span class="p">,</span> <span class="n">tableau</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tableau</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="n">valeur</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">True</span>
    <span class="k">return</span> <span class="bp">False</span>
</code></pre></div></div>

<p>L’intérieur de la boucle est constitué du code :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="n">valeur</span><span class="p">:</span>
    <span class="k">return</span> <span class="bp">True</span>
</code></pre></div></div>

<p>Qui est de complexité $\mathcal{O}(1)$. Ce code est exécuté autant de fois que l’on va rentrer dans la boucle for. La complexité de notre algorithme est alors égale à $k * \mathcal{O}(1)$ où $k$ est le nombre de fois où l’on rentre dans la boucle.</p>

<p>On cherche le cas le pire. Elle est atteinte lorsque la boucle for parcours tout le tableau, c’est à dire pour deux cas :</p>

<ul>
  <li>l’élément recherché n’est pas dans le tableau</li>
  <li>l’élément recherché est le dernier élément du tableau</li>
</ul>

<p>On en conclut que la complexité de notre algorithme est $n * \mathcal{O}(1)$ où $n$ est la taille du tableau qui est un paramètre d’entrée (c’est donc une variable qu’on ne peut faire disparaître) la complexité de notre algorithme est : $\mathcal{O}(n)$.</p>

<p>La complexité minimale est quant-à-elle atteinte lorsque l’on ne parcours pas notre boucle, c’est à dire lorsque la valeur recherchée est la 1ère valeure du tableau. Dans ce cas là, la complexité est de $\mathcal{O}(1)$ opérations.</p>

<p>Au final :</p>

<ul>
  <li>la complexité maximale de l’algorithme <code class="language-plaintext highlighter-rouge">est_dans_tableau</code> est $\mathcal{O}(n)$</li>
  <li>la complexité minimale de l’algorithme <code class="language-plaintext highlighter-rouge">est_dans_tableau</code> est $\mathcal{O}(1)$</li>
</ul>

<h2 id="types-de-complexité-en-algorithmie">types de complexité en algorithmie</h2>

<p>En algorithmie, la plupart des complexités que l’on étudiera seront de cinq types (plus leurs combinaisons) :</p>

<blockquote class="note">
  <p>On appelle :</p>

  <ul>
    <li>
<strong>complexité constante</strong> une complexité en $\mathcal{O}(1)$</li>
    <li>
<strong>complexité logarithmique</strong> une complexité en $\mathcal{O}(\ln(n))$ où $n$ est le paramètre de taille de l’algorithme</li>
    <li>
<strong>complexité linéaire</strong> une complexité en $\mathcal{O}(n)$ où $n$ est le paramètre de taille de l’algorithme</li>
    <li>
<strong>complexité polynomiale</strong> une complexité en $\mathcal{O}(n^k)$ où $n$ est le paramètre de taille de l’algorithme et $k$ une constante</li>
    <li>
<strong>complexité exponentielle</strong> une complexité en $\mathcal{O}(k^n)$ où $n$ est le paramètre de taille de l’algorithme et $k$ une constante</li>
  </ul>

</blockquote>

<p>Les type de complexité ci-dessus sont rangés par taille, de la moins longue à la plus longue. Remarquez qu’un algorithme de complexité linaire nécessite de lire toutes les données au plus un nombre constant de fois pour s’exécuter. Un algorithme de complexité logarithmique n’a même pas besoin de lire une fois toutes les données pour s’exécuter ! Ceci n’est souvent possible que si les données en entrées ont une structure très particulière. Par exemple pour le problème de la recherche du plus grand élément d’une liste :</p>

<ul>
  <li>trouver le plus grand élément dans une liste non triée nécessite $\mathcal{O}(n)$ où $n$ est la taille de la liste,</li>
  <li>trouver le plus grand élément dans une liste triée nécessite $\mathcal{O}(1)$ où $n$ est la taille de la liste,</li>
</ul>

<p>Ou le problème de la recherche d’un élément particulier de la liste :</p>

<ul>
  <li>trouver un élément dans une liste non triée nécessite $\mathcal{O}(n)$ où $n$ est la taille de la liste,</li>
  <li>trouver un élément dans une liste triée nécessite $\mathcal{O}(\ln (n))$ où $n$ est la taille de la liste en utilisant la <a href="https://fr.wikipedia.org/wiki/Recherche_dichotomique">recherche dichotomique</a>
</li>
</ul>

<blockquote>
  <p>Notez bien que la complexité logarithmique est la même quelque soit la base utilisée. En effet $\log_k(n) = \frac{\ln (n)}{\ln (k)}$ et donc $\mathcal{O}(\log_k(n)) = \mathcal{O}(\ln(n))$ pour toute base constante $k$.</p>
</blockquote>

<p>Il est crucial de chercher la meilleure complexité pour un algorithme car ses performance seront drastiquement différentes selon le type de complexité qu’il possède, comme le montre les deux tableaux ci-dessous, repris du livre <a href="https://en.wikipedia.org/wiki/Computers_and_Intractability">Computer and intractabilityt</a>. Ce qu’il faut retenir :</p>

<blockquote class="note">
  <ul>
    <li>il y a une <strong>énorme différence</strong> entre complexité linéaire et complexité polynomiale</li>
    <li>il y a une <strong>énorme différence</strong> entre complexité polynomiale et complexité exponentielle (qu’il ne faut donc jamais avoir si possible)</li>
  </ul>
</blockquote>

<h3 id="temps-pour-résoudre-un-problème-de-taille-n">temps pour résoudre un problème de taille $n$</h3>

<p>Exemple d’évolution du temps de calcul par rapport à la complexité. En supposant, que l’on ait un ordinateur qui résout des problèmes de complexité $n$ en 0.01 ms pour des données de taille 10, on peut remplir le tableau ci-après.</p>

<p>En colonnes le nombre $n$ de données, en lignes les complexités des algorithmes.</p>

<table>
  <thead>
    <tr>
      <th>complexité</th>
      <th>10</th>
      <th>20</th>
      <th>30</th>
      <th>40</th>
      <th>50</th>
      <th>60</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\ln(n)$</td>
      <td>2 $\mu s$</td>
      <td>3 $\mu s$</td>
      <td>3 $\mu s$</td>
      <td>4 $\mu s$</td>
      <td>4 $\mu s$</td>
      <td>4 $\mu s$</td>
    </tr>
    <tr>
      <td>$n$</td>
      <td>0.01 ms</td>
      <td>0.02 ms</td>
      <td>0.03 ms</td>
      <td>0.04 ms</td>
      <td>0.05 ms</td>
      <td>0.06 ms</td>
    </tr>
    <tr>
      <td>$n^2$</td>
      <td>0.1 ms</td>
      <td>0.4 ms</td>
      <td>0.9 ms</td>
      <td>1.6 ms</td>
      <td>2.5 ms</td>
      <td>3.6 ms</td>
    </tr>
    <tr>
      <td>$n^3$</td>
      <td>1 ms</td>
      <td>8 ms</td>
      <td>27 ms</td>
      <td>64 ms</td>
      <td>125 ms</td>
      <td>216 ms</td>
    </tr>
    <tr>
      <td>$n^5$</td>
      <td>1s</td>
      <td>3.2 s</td>
      <td>24.3 s</td>
      <td>1.7 min</td>
      <td>5.2 min</td>
      <td>13 min</td>
    </tr>
    <tr>
      <td>$2^n$</td>
      <td>1 ms</td>
      <td>1s</td>
      <td>17.9 min</td>
      <td>12.7 jours</td>
      <td>35.7 ans</td>
      <td>36600 ans</td>
    </tr>
    <tr>
      <td>$3^n$</td>
      <td>59 ms</td>
      <td>58 min</td>
      <td>6.5 ans</td>
      <td>385500 ans</td>
      <td>$2.27\cdot 10^8$ siècles</td>
      <td>$1.3\cdot 10^{13}$ siècles</td>
    </tr>
  </tbody>
</table>

<p>L’évolution est dramatique plus la complexité augmente. Pour une complexité logarithmique, le temps <em>semble</em> constant et pour une complexité polynomiale, la croissance reste maitrisée même s’il vaut mieux avoir une petite complexité pour traiter plus de données. Pour une complexité exponentielle ($2^n$ et $3^n$) en revanche, la durée est tout simplement rédhibitoire.</p>

<blockquote>
  <p>Pour générer le tableau, on voit que le temps  $t$ pour exécuter 1 opération est de .001ms (on regarde la ligne de complexité linéaire : pour $n=10$ on prend 0.01 opérations, donc 1 opération nécessite $0.01/10ms$). Le temps pris pour exécuter $f(n)$ opérations avec une entrée de taille de $n$ est alors : $t \cdot f(n)$</p>
</blockquote>

<h3 id="nombre-de-problèmes-résolus-par-heure">nombre de problèmes résolus par heure</h3>

<p>En colonne la rapidité de la machine, en ligne la taille maximale d’un problème que l’on peut résoudre en 1heure.</p>

<table>
  <thead>
    <tr>
      <th>complexité</th>
      <th>machine actuelle</th>
      <th>100x plus rapide</th>
      <th>1000x plus rapide</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\ln(n)$</td>
      <td>$N0$</td>
      <td>$e^{100} \cdot N0$</td>
      <td>$e^{1000} \cdot N0$</td>
    </tr>
    <tr>
      <td>$n$</td>
      <td>$N1$</td>
      <td>$100 \cdot N1$</td>
      <td>$1000 \cdot N1$</td>
    </tr>
    <tr>
      <td>$n^2$</td>
      <td>$N2$</td>
      <td>$10 \cdot N2$</td>
      <td>$31.6 \cdot N2$</td>
    </tr>
    <tr>
      <td>$n^3$</td>
      <td>$N3$</td>
      <td>$4.64 \cdot N3$</td>
      <td>$10 \cdot N3$</td>
    </tr>
    <tr>
      <td>$n^5$</td>
      <td>$N4$</td>
      <td>$2.5 \cdot N4$</td>
      <td>$3.98 \cdot N4$</td>
    </tr>
    <tr>
      <td>$2^n$</td>
      <td>$N5$</td>
      <td>$N5 + 6.64$</td>
      <td>$N5 + 9.97$</td>
    </tr>
    <tr>
      <td>$3^n$</td>
      <td>$N6$</td>
      <td>$N6 + 4.19$</td>
      <td>$N6 + 6.29$</td>
    </tr>
  </tbody>
</table>

<p>La encore, l’évolution est dramatique plus la complexité augmente. Pour des complexités logarithmiques et polynomiales le nombre de problème augmente d’un facteur multiplicatif lorsque la vitesse augmente, mais ce n’est pas le cas pour des complexités exponentielles. Pour ces problèmes, augmenter la vitesse de la machine ne change pas fondamentalement le nombre de problèmes que l’on peut résoudre.</p>

<blockquote>
  <p>Pour générer le tableau, on suppose que l’on peut résoudre $K$ opérations en 1 heure. On cherche alors $n$ tel que $f(n)$ soit égal à $K$ et donc $n = f^{-1}(K)$. En remarquant que $K$ est égal à la taille maximale d’un problème de complexité linéaire résoluble en 1heure, on la taille maximale $n$ d’un problème de complexité $f(n)$ résoluble en 1 heure pour une machine allant $k$ fois pus vite qu’une machine actuelle vaut $f^{-1}(k \cdot N1)$.</p>
</blockquote>

<h3 id="le-cas-particulier-de-n">le cas particulier de n!</h3>

<p>Souvent les étudiants veulent que leurs algorithmes soient en $\mathcal{O}(n!)$. Ce n’est <strong>presque jamais exact</strong> ! En effet, la <a href="https://fr.wikipedia.org/wiki/Formule_de_Stirling">formule de sirling</a> donne l’équivalent suivant pour $n!$ :</p>

\[n! \sim \sqrt{2\pi n}(\frac{n}{e})^n\]

<p>On a donc que $n!$ est de l’ordre de $\mathcal{O}(n^{n+1/2})$, qui est vachement plus grand que $\mathcal{O}(2^{n})$ qui est déjà gigantesque.</p>

<blockquote class="note">
  <p>Si vous pensez que votre algorithme tout bête est en $\mathcal{O}(n!)$. Réfléchissez-y à deux fois. C’est presque sûrement une erreur…</p>
</blockquote>

<h2 id="règles-de-calcul-de-complexité">règles de calcul de complexité</h2>

<p>On va donner ici quelques règles de calcul de complexité pour que vous puissiez estimer rapidement la complexité d’un algorithme simple.</p>

<h3 id="une-boucle-simple">une boucle simple</h3>

<p>Lorsque l’on a une boucle où le nombre de fois où l’on va rentrer dedans est évident.</p>

<p>Par exemple :</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
tant que condition:
    bloc d'instructions

</code></pre></div></div>

<blockquote class="note">
  <p>La complexité est : $\mathcal{O}$(nombre de fois ou la condition est remplie) * ($\mathcal{O}$(complexité du bloc d’instruction) + $\mathcal{O}$(complexité de la vérification de la condition))</p>
</blockquote>

<p>Souvent, $\mathcal{O}$(complexité de la vérification de la condition) sera égal à $\mathcal{O}(1)$ et pourra ne pas en tenir compte dans le calcul. C’est le cas, entre autre pour une boucle tant que :</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
pour chaque element de structure:
    bloc d'instructions

</code></pre></div></div>

<blockquote class="note">
  <p>La complexité est : $\mathcal{O}$(nombre d’éléments de la structure) * $\mathcal{O}$(complexité du bloc d’instruction)</p>
</blockquote>

<p>Si le bloc d’instructions est une suite d’instructions de complexité $\mathcal{O}(1)$, on pourra ne pas en tenir compte dans le calcul et la complexité est alors égale à la taille de la structure.</p>

<p>En conclusion :</p>

<blockquote class="note">
  <p>Si le bloc d’instruction est une suite d’instructions de complexité $\mathcal{O}(1)$ et que la vérification de la fin de la boucle est $\mathcal{O}(1)$, la complexité de la boucle est égal au nombre de fois où l’on effectue la boucle</p>
</blockquote>

<h3 id="boucles-imbriquées-indépendantes">boucles imbriquées indépendantes</h3>

<p>Plusieurs boucles imbriquées dont dont le nombre de fois où l’on va rentrer dedans est indépendant des autres boucles. Par exemple :</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>boucle 1 éxecutée n1 fois:
    boucle 2 éxecutée n2 fois:
        ...
            boucle i éxecutée ni fois:
                bloc d'instructions
</code></pre></div></div>

<p>On peut utiliser la règle précédente de façon récursive, la partie $\mathcal{O}$(complexité du bloc d’instruction) contenant elle même une ou plusieurs boucles.</p>

<blockquote class="note">
  <p>Si la condition à remplir pour rentrer dans la boucle est en $\mathcal{O}(1)$, la complexité des boucles imbriquées est le produit du nombre de fois où l’on rentre dans chaque boucle pris indépendamment multiplié par la complexité du bloc d’instructions.</p>
</blockquote>

<p>Exemple :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">total</span><span class="o">=</span><span class="mi">0</span>
<span class="n">de</span> <span class="n">i</span><span class="o">=</span><span class="mi">1</span> <span class="n">à</span> <span class="n">n</span><span class="o">-</span><span class="mi">1</span> <span class="n">faire</span><span class="p">:</span>
    <span class="n">de</span> <span class="n">j</span><span class="o">=</span><span class="mi">1</span> <span class="n">à</span> <span class="n">n</span> <span class="n">faire</span> <span class="p">:</span>
        <span class="n">total</span><span class="o">=</span><span class="n">total</span><span class="o">+</span><span class="mi">1</span>
<span class="n">Rendre</span> <span class="n">total</span>
</code></pre></div></div>

<p>La boucle en $i$ est exécuté $n-1$ fois ($i$ va de 1 à $n-1$), donc $\mathcal{O}(n)$ fois. La boucle en $j$ va également être exécutée $\mathcal{O}(n)$ fois indépendamment de la boucle en $i$. Enfin la complexité du bloc d’instruction est $\mathcal{O}(1)$, la complexité totale des deux boucles imbriquées vaut :</p>

\[{
\underbrace{\mathcal{O}(n)}_{\mbox{boucle en i}} \cdot \underbrace{\mathcal{O}(n)}_{\mbox{boucle en j}} \cdot \underbrace{\mathcal{O}(1)}_{\mbox{bloc d'instructions}}
} = \mathcal{O}(n^2)\]

<blockquote>
  <p>Ne comptez pas trop précisément le nombre de fois où l’on rentre dans une boucle $n-3$ exécution de la boucle pouvant être avantageusement remplacé par $\mathcal{O}(n)$</p>
</blockquote>

<h3 id="boucles-dépendantes-mais-monotones">boucles dépendantes mais monotones</h3>

<p>Il arrive souvent que les boucles imbriquées d’un algorithme soient dépendantes les unes des autres. Dans le cas général on ne peut pas factoriser le calcul de la complexité et il faut alors dérouler tout l’algorithme en additionnant les complexités de chaque ligne comme s’il n’y avait pas de boucles.</p>

<p>Il existe cependant un cas pratique (et qui arrive assez souvent) où l’on peut factoriser :</p>

<blockquote class="note">
  <p>Si une boucle s’exécute un nombre variable de fois, mais que cette variation est croissante (respectivement décroissante), on peut considérer pour le calcul de la complexité qu’elle s’exécute à chaque fois de l’ordre du maximum de fois.</p>
</blockquote>

<p>On va vérifier cela avec un exemple :</p>

<style>
    table, td, tr, th, pre {
        padding:0;
        margin:0;
        border:none
    }
</style>

<figure class="highlight"><pre><code class="language-text" data-lang="text"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td>
<td class="code"><pre>total=0
de i=1 à n-1 faire :
    de j=i+1 à n faire :
        total=total+1
Rendre total
</pre></td>
</tr></tbody></table></code></pre></figure>

<p>Le nombre de fois où la boucle en $j$ est exécutée est un nombre variable de fois qui dépend de la valeur de $i$. Comme $i$ va croitre, le nombre de fois où cette boucle va s’exécuter va décroitre. Si l’on applique la règle  on peut dire qu’elle va s’exécuter de l’ordre de $\mathcal{O}(n)$ fois comme dans l’exemple de la partie précédente. La complexité de l’algorithme est donc de $\mathcal{O}(n^2)$.</p>

<p>Refaisons le calcul en décomposant toutes les instructions, comme on le ferait dans le cas général, pour voir que notre règle est valide (et donnera aussi une idée de la preuve de cette règle) :</p>

<ul>
  <li>ligne 1 : $\mathcal{O}(1)$</li>
  <li>itération pour $i=1$:
    <ul>
      <li>une affectation $i=1$ : $\mathcal{O}(1)$</li>
      <li>boucle pour $j=1$:
        <ul>
          <li>une affectation de $j$ :  $\mathcal{O}(1)$</li>
          <li>la ligne 4 :  $\mathcal{O}(1)$</li>
          <li>le tout $n-1$ fois</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>itération pour $i=2$:
    <ul>
      <li>une affectation $i=2$ : $\mathcal{O}(1)$</li>
      <li>boucle pour $j=2$:
        <ul>
          <li>une affectation de $j$ :  $\mathcal{O}(1)$</li>
          <li>la ligne 4 :  $\mathcal{O}(1)$</li>
          <li>le tout $n-2$ fois</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>…</li>
  <li>itération pour $i=n-1$:
    <ul>
      <li>une affectation $i=n-1$ : $\mathcal{O}(1)$</li>
      <li>boucle pour $j=n-1$:
        <ul>
          <li>une affectation de $j$ :  $\mathcal{O}(1)$</li>
          <li>la ligne 4 :  $\mathcal{O}(1)$</li>
          <li>le tout $1$ fois</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>ligne 5 : $\mathcal{O}(1)$</li>
</ul>

<p>Notre complexité totale est donc :</p>

\[\begin{aligned}
    \mathcal{O}(1) + \\
    (\mathcal{O}(1) + (n-1) \cdot (\mathcal{O}(1) + \mathcal{O}(1))) + \\
    (\mathcal{O}(1) + (n-2) \cdot (\mathcal{O}(1) + \mathcal{O}(1))) + \\
    \dots\\
 + (\mathcal{O}(1) + (1) \cdot (\mathcal{O}(1) + \mathcal{O}(1))) \\
 + \mathcal{O}(1)
\end{aligned}\]

<p>comme $\mathcal{O}(1) + \mathcal{O}(1) = \mathcal{O}(1)$, on a :</p>

\[\begin{aligned}
    \mathcal{O}(1) + \\
    (\mathcal{O}(1) + (n-1) \cdot \mathcal{O}(1)) + \\
    (\mathcal{O}(1) + (n-2) \cdot \mathcal{O}(1)) + \\
    \dots\\
 + (\mathcal{O}(1) + 1 \cdot \mathcal{O}(1)) \\
 + \mathcal{O}(1)
\end{aligned}\]

<p>Ce qui donne :</p>

\[\begin{aligned}
    \mathcal{O}(1) + \\
    n \cdot \mathcal{O}(1) + \\
    (n-1) \cdot \mathcal{O}(1) + \\
    \dots\\
 + \mathcal{O}(1)
\end{aligned}\]

<p>et donc notre complexité vaut :
\(\mathcal{O}(1) + \sum_{1\leq i \leq n} i \cdot \mathcal{O}(1)\)</p>

<p>Comme la somme des n premiers entiers vaut $\frac{(n+1)(n)}{2}$ notre complexité devient :</p>

\[\mathcal{O}(1) + \frac{(n+1)(n)}{2} \mathcal{O}(1)\]

<p>Ce qui est de l’ordre de : $\mathcal{O}(\frac{(n+1)(n)}{2})$. Or :</p>

\[\mathcal{O}(\frac{(n+1)(n)}{2}) = \mathcal{O}(\frac{n^n + n}{2}) = \mathcal{O}(n^2 +n) = \mathcal{O}(n^2)\]

<p>On retrouve bien le résultat attendu.</p>

<h3 id="complexité-dalgorithmes-récursifs">complexité d’algorithmes récursifs</h3>

<p>Un algorithme récursif est un algorithme qui s’appelle lui-même jusqu’à ce qu’on arrive à une condition d’arrêt qui stope la récursion. On en calcul la complexité en posant une équation qu’il faut résoudre :</p>

<blockquote class="note">
  <p>Pour calculer la complexité d’un algorithme récursif en fonction de la taille $n$ de l’entrée, on pose que $C(n)$ est la complexité et l’on utilise cette fonction pour estimer la complexité des appels récursifs. Une fois les complexités des éléments d’arrêts estimés, trouver $C(n)$ revient à résoudre une équation de récurrence.</p>
</blockquote>

<p>Pour ilustrer ce calcul, prenons l’exemple suivant :</p>

<style>
    table, td, tr, th, pre {
        padding:0;
        margin:0;
        border:none
    }
</style>

<figure class="highlight"><pre><code class="language-text" data-lang="text"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td>
<td class="code"><pre>fonction maximum(t, n):
    si n == 1
        rendre t[0]
    sinon:
        x = maximum(t, n-1)
        si x &gt; t[n-1]:
            rendre x
        sinon:
            rendre t[n-1]
</pre></td>
</tr></tbody></table></code></pre></figure>

<p>On exécute cette fonction avec comme paramètres initiaux un tableau nommé <code class="language-plaintext highlighter-rouge">t</code> de taille <code class="language-plaintext highlighter-rouge">n</code>. On vérifie qu’avec ces paramètres initaux :</p>

<ol>
  <li>l’algorithme converge bien</li>
  <li>il rend bien le maximum de <code class="language-plaintext highlighter-rouge">t</code>
</li>
</ol>

<p>La taille des données est de l’ordre de la taille du tableau, c’est à dire le paramètre $n$. On pose alors que la complexité de notre algorithme pour un tableau de taille $n$ est : $C(n)$. De là :</p>

<ul>
  <li>la complexité de la ligne 2 est en $\mathcal{O}(1)$ : c’est une comparaison</li>
  <li>la complexité de la ligne 3 est en $\mathcal{O}(1)$ : on cheche un élément particulier d’un tableau</li>
  <li>la complexité de la ligne 5 est en $C(n-1) + \mathcal{O}(1)$ : on exécute notre algorithme avec un tableau de taille $n-1$ — sa complexité est donc par définition de $C(n-1)$ — puis on affecte le résultat à une variable</li>
  <li>la complexité de la ligne 6 est en $\mathcal{O}(1)$ : c’est une comparaison d’une varaible et d’un élément particulier d’un tableau</li>
  <li>la complexité de la ligne 7 est en $\mathcal{O}(1)$</li>
  <li>la complexité de la ligne 9 est en $\mathcal{O}(1)$ : on rend un élément particulier d’un tableau</li>
</ul>

<p>La complexité est définie par l’équation de récurrence $C(n) = \mathcal{O}(1) + C(n-1)$. Notre condition d’arrêt est obtenue pour <code class="language-plaintext highlighter-rouge">n</code> valant 1 et dans ce cas on a $C(1) = \mathcal{O}(1)$</p>

<p>Trouver $C(n)$ revient à résoudre :</p>

\[\left\{
    \begin{array}{lcl}
        C(n) &amp; = &amp; \mathcal{O}(1) + C(n-1)\\
        C(0) &amp; = &amp; \mathcal{O}(1)
    \end{array}
\right.\]

<p>On a alors :</p>

\[\begin{array}{lcl}
    C(n) &amp; = &amp; \mathcal{O}(1) + C(n-1)\\
    &amp; = &amp; \mathcal{O}(1) + \mathcal{O}(1) + C(n-2) = 2 \cdot \mathcal{O}(1) + C(n-2)\\
    &amp; = &amp; 3 \cdot \mathcal{O}(1) + C(n-3) \\
    &amp; = &amp; \dots \\
    &amp; = &amp; i \cdot \mathcal{O}(1) + C(n-i) \\
    &amp; = &amp; \dots \\
    &amp; = &amp; (n-1) \cdot \mathcal{O}(1) + C(1) = (n-1) \cdot \mathcal{O}(1) + \mathcal{O}(1) \\
    &amp; = &amp; n \cdot \mathcal{O}(1) = \mathcal{O}(n) \\
\end{array}\]

<p>Au fnal, on trouve que la complexité $C(n)$ de notre algorithme est en $\mathcal{O}(n)$ où $n$ est la taille du tableau placé initialement en paramètre.</p>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/cours_informatique/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">cours d'informatique</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">cours d'informatique</li>
</ul>
      </div>

      <div class="footer-col footer-col-2">
<ul class="social-media-list"><li><a href="https://github.com/FrancoisBrucker"><svg class="svg-icon"><use xlink:href="/cours_informatique/assets/minima-social-icons.svg#github"></use></svg> <span class="username">FrancoisBrucker</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Support de cours/td d'informatique à l'école centrale marseille.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
